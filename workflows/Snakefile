import numpy as np
import pandas as pd
import yaml
import shutil

configfile: "config/config.yaml"
wildcard_constraints:
    method="coarsening|sparsification"

def cartesian_product(*arrays):
    '''Generates a cartesian product from a group of arrays.

    Parameters
    ----------
        arrays : np.ndarray or list
            A sequence of arrays.
    
    Returns
    -------
        cart_prod : np.ndarray
            2D numpy array containing all possible combinations of values
            from the individual arrays.
    '''
    meshgrids = np.meshgrid(*arrays,indexing='ij')
    cart_prod = np.stack(meshgrids,axis=-1)
    cart_prod = cart_prod.reshape(-1,len(arrays))
    return cart_prod

def generate_samples(params):
    '''Generates the samples for the method.

    Generates the set of parameters used to generate each graph reduction,
    based on the specified values for each individual parameters. The 
    resulting sets of parameters are equivalent to a cartesian products over
    the values of each indivial parameter.

    Parameters
    ----------
        method_config : dict of str to str
            Dictionary of 'values' for each parameter. Keys correspond
            to the name of the parameter used by the generator, while
            the corresponding values are strings specifying a numpy 
            operation to generate the vector of values for the parameter.

    Returns
    -------
        samples : pandas.DataFrame
            Dataframe with reduction parameters for each graph, indexed 
            by the graph id.
    '''
    samples = cartesian_product(*[eval(val) for val in params.values()])
    samples = pd.DataFrame(samples,columns=params.keys())

    return samples

def load_dict(file):
    '''Loads a dictionary from a YAML file.

    Parameters
    ----------
        file : str
            Path to YAML file containing the dictionary.

    Returns
    -------
        dic : dict
            Dictionary read from file.
    '''
    with open(file,'r') as f:
        dic = yaml.safe_load(f)

    return dic

# Retrieve methods to analyze
methods = config['methods'].keys()

# Generate the samples for each method
samples = {method: generate_samples(config['methods'][method]['parameters']) for method in methods}

rule all:
    input:
        expand("data/networks/hamsterster/giant/miniaturization/parameters_{id}.yaml",id=range(samples['miniaturization'].shape[0]))
        
rule miniaturization_params:
    '''Calculates the optimal miniaturization parameters of a graph
    
    Input:
        - YAML file containing the metrics of the graphs to miniaturize

    Output:
        - YAML file containg the optimal weights and inverse temperature to miniaturize
          the graph to the specified size
    '''
    input:
        "data/networks/{graph}/metrics.yaml"
    output:
        "data/networks/{graph}/miniaturization/parameters_{id}.yaml"
    resources:
        runtime=400,
        mem_mb=2000
    log:
        "logs/{graph}/reduction/miniaturization/parameters_{id}/parameters.log"
    params:
        params=lambda w:samples['miniaturization'].loc[int(w.id)],
        n_trials=30,
        n_steps=100
    script:
        "scripts/miniaturization_params.py"

rule miniaturization:
    '''Miniaturizes a graph using the parallel tempering algorithm

    Input:
        - YAML file containing the of the graph
        - YAML file containing the miniaturization weights and inverse temperature
    
    Output:
        - Scipy Sparse Array representing the adjacency matrix of the miniature
        - Directory of parquet files containing the individual trajectories of each
          replica of the parallel tempering algorithm
    '''
    input:
        "data/networks/{graph}/metrics.yaml",
        lambda w: f"data/networks/{w.graph}/miniaturization/parameters_{int(w.idx) // config['methods']['miniaturization']['n_graphs']}.yaml"
    output:
        "data/networks/{graph}/miniaturization/graph_{idx}/graph_adjacency.npz",
        directory("data/networks/{graph}/miniaturization/graph_{idx}/trajectories")
    params:
        params=lambda w:samples['miniaturization'].loc(int(w.id))
    resources:
        runtime=1440,
        mem_mb=2000,
        tasks=6,
        mpi="mpiexec"
    conda:
        "envs/miniaturize.yaml"
    log:
        "logs/{graph}/reduction/miniaturization/graph_{idx}.log"
    shell:
        "{resources.mpi} -n {resources.tasks} scripts/miniaturize.py"
        " {input[0]} {input[1]} {output[0]} {output[1]} {params.shrinkage}"
        " --n_changes {params.params['n_changes']}"
        " --n_steps {params.params['n_steps']}"
        " --n_substeps {params.params['n_sub_steps']}"
        " --log-file {log[0]}"

rule reduce:
    '''Reduce a graph dataset.

    This rule reduces the graph dataset using via sparsification or coarsening, 
    depending on the wildcard specified. 

    Input:
        - Scipy Sparse Array containing the adjacency matrix of the graph to reduce
    
    Output:
        - Scipy Sparse Array containing the adjacency matrix of the reduced graph
        - YAML file containing the set of parameters used to reduce the graph
    '''
    input:
        "data/networks/{graph}/graph_adjacency.npz",
    output:
        "data/networks/{graph}/{method}/{graph_idx}/graph_adjacency.npz",
        "data/networks/{graph}/{method}/{graph_idx}/params_reduction.yaml"
    log:
        "logs/reduction_{graph}/{method}/{graph_idx}.log"
    message:
        "Reducing {wildcards.graph} ({wildcards.graph_idx}) using {wildcards.method}..."
    params:
        method_parameters=lambda w: samples[w.method].loc[w.graph_idx].to_dict()
    resources:
        runtime=120,
        mem_mb=1000
    script:
        "scripts/graph_reduce.py"

rule characterize:
    '''Characterizes a graph by calculating its relevant metrics

    Input:
        - Scipy Sparse Array containing the adjacency matrix of the graph
    
    Output:
        - YAML file containing the network metrics of the graph
    '''
    input:
        "{path}/graph_adjacency.npz"
    output:
        "{path}/graph_metrics.yaml"
    script:
        "scripts/graph_characterize.py"

rule distribution:
    '''Calculates the distribution over some node property

    The degree distribution of the pairwise-distance distribution of graph
    nodes is calculated. 

    Input:
        - Scipy Sparse Array containing the adjacency matrix of the graph

    Output:
        - 2-D Numpy Array representing the PDF of the specified property.
          The first column contains that the prroperty takes. The second
          column are the frequencies for each value.
    '''
    input:
        "{path}/graph_adjacency.npz"
    output:
        "{path}/distribution_{quantity}.npy"
    script:
        "scripts/graph_distribution.py"

rule distribution_metrics:
    '''Calculates metrics from the specified distribution. 

    Calculates relevant measures of central tendency as well as some 
    low order moments of the distribution.

    Input:
        - Numpy Array containing the distribution.

    Output:
        - YAML file containing the requested metrics.
    '''
    input:
        "{path}/distribution_{quantity}.npy"
    output:
        "{path}/distribution_{quantity}.yaml"
    script:
        "scripts/graph_distribution_metrics.py"

rule simulate:
    '''Simulates the diffusion model on the graph.

    For the SIR model, the trajectories returned by the rule are the 
    normalized populations for each compartment at every iteration.

    Input:
        - Scipy Sparse Array containing the adjacency matrix of the graph.

    Output:
        - Numpy Array containing the relevant trajectories of the model.
    '''
    input:
        "{path}/graph_adjacency.npz",
    output:
        "{path}/trajectories_{model}.npy"
    params:
        config=lambda w:config['models'][w.model]
    resources:
        runtime=60,
        mem_mb=1000,
        cpus_per_task=1,
    script:
        "scripts/simulation_{wildcards.model}_run.py"

rule plot_sir:
    '''Plots the trajectories of the SIR model.

    Input:
        - Numpy Array of SIR trajectories.

    Output:
        - PNG file with an SIR plot.
    '''
    input:
        "{path}/trajectories_sir.npy"
    output:
        "{path}/trajectories_sir.png"
    script:
        "scripts/sir_plot.py"

rule qois_sir:
    '''Calculates the Quantities of Interest of the SIR model.

    The QOIs include total epidemic size, peak of number of infected
    people, and time-to-peak.

    Input:
        - Numpy array of SIR trajectories.

    Output:
        - YAML file with the QOIs of the SIR model.
    '''
    input:
        "{path}/trajectories_sir.npy"
    output:
        "{path}/qois_sir.yaml"
    script:
        "scripts/model_qois_sir.py"

rule draw_original:
    input:
        "data/networks/{graph}/drawing.png"
    output:
        "results/{graph}/drawing.png"
    shell:
        """
        mv {input[0]} {output[0]}
        """

rule gexf:
    '''Generates a gexf file of a graph.

    Stores the graph in the .gexf format compatible with gephi.

    Input:
        - Scipy Sparse Array containing the adjacency matrix of a grpah.

    Output:
        - .gexf file of the graph.
    '''
    input:
        "{path}/graph_adjacency.npz"
    output:
        "{path}/graph.gexf"
    script:
        "scripts/graph_gexf.py"

rule plot_graph:
    '''Creates a visualization of the graph dataset.

    Utilizes the ForceAtlas2 algorithm to generate the layout on the graph
    and draw it on a plane.

    Input:
        - Scipy Sparse Array containing the adjacency matrix of the graph.

    Output:
        - PNG file with a visualization of the graph dataset
    '''
    input:
        "{path}/graph_adjacency.npz"
    output:
        "{path}/drawing.png"
    log:
        "{path}/drawing.log"
    conda:
        "envs/viz.yaml"
    shell:
        """
        python -u scripts/graph_draw.py {input[0]} {output[0]} &> {log}
        """
